{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef005ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f077bc7",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2cd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "#observation, info = env.reset()\n",
    "\n",
    "#for _ in range(100):\n",
    "#    action = env.action_space.sample()\n",
    " #   observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "  #  if terminated or truncated:\n",
    "        \n",
    "   #     observation, info = env.reset()\n",
    "#env.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506e5f1",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2487521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1e4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 128) # 4 is length of states vector, 2 is number of actions\n",
    "        self.layer2 = nn.Linear(128,128)\n",
    "        self.layer3 = nn.Linear(128, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "    def act(self, state): # I assume batch contains only 1 vector, otherwise it should be changed\n",
    "        y = self(state) # vector of length 2\n",
    "        return torch.argmax(y).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3613401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = 1\n",
    "    \n",
    "    def act(self, state):\n",
    "        return np.random.randint(0,2)\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39ce5f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43c528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch():\n",
    "    def __init__(self):\n",
    "        self.l = []\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l)\n",
    "\n",
    "    def empty(self):\n",
    "        self.l = []\n",
    "\n",
    "    def push(self, element):\n",
    "        self.l.append(element)\n",
    "    def __iter__(self):\n",
    "        return iter(self.l)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f87a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_batch(model, env, optimizer, batch):\n",
    "    # batch is list of tuples\n",
    "\n",
    "    #states = torch.stack([t[0] for t in batch.l])\n",
    "    #predicted = model(states)\n",
    "    loss = 0.0\n",
    "    huber_loss = torch.nn.HuberLoss()\n",
    "\n",
    "    for sample in batch:\n",
    "        obs_0, a, r, obs_1 = sample[0], sample[1], sample[2], sample[3]\n",
    "        predicted = model(obs_0)[a]\n",
    "        target = r + model(obs_1).max() \n",
    "        loss += huber_loss(predicted, target)\n",
    "    loss = loss / len(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e59fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ca7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, game, state, eps):\n",
    "    \n",
    "    if np.random.rand() < eps:\n",
    "        return game.action_space.sample() \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy.act(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae04368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, game, optimizer, n_ep = 10000, batch_size = 32, eps = 0.01, gamma = 1.0):\n",
    "     \n",
    "    # gamma is decay rate\n",
    "\n",
    "    batch = Batch()\n",
    "\n",
    "    for ep in range(n_ep):\n",
    "        obs_0, info = game.reset()\n",
    "        obs_0 = torch.tensor(obs_0, dtype = torch.float32)\n",
    "        duration = 0 # duration of episode\n",
    "        total_r_ep = 0.0 # total rewards per episode\n",
    "\n",
    "        flag = False # if termination happens while we fill batch, after optimization step we finish episode.\n",
    "\n",
    "        while True:\n",
    "\n",
    "            while len(batch) < batch_size:\n",
    "                a = select_action(model, game, obs_0, eps)\n",
    "                obs_1, r, terminated, truncated, info = game.step(a)\n",
    "                obs_1 = torch.tensor(obs_1, dtype = torch.float32)\n",
    "                sample = (obs_0, a, r, obs_1)\n",
    "                batch.push(sample)\n",
    "\n",
    "                obs_0 = obs_1.clone()\n",
    "\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    obs_0, info = game.reset()\n",
    "                    obs_0 = torch.tensor(obs_0, dtype = torch.float32)\n",
    "                    flag = True\n",
    "                duration += 1 # shesacvlelia\n",
    "                total_r_ep += r # shesacvlelia\n",
    "            \n",
    "\n",
    "            # now we train on batch \n",
    "            optimize_batch(model, game, optimizer, batch)\n",
    "            batch.empty()\n",
    "            if flag:\n",
    "                if ep%100 == 0:\n",
    "                    print(f\"duration of episode {ep} was {duration}\")\n",
    "                    print(f\"total reward for episode {ep} was {total_r_ep}\")\n",
    "                break\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb38b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "linear = net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b830e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (layer1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ee5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(linear.parameters(), lr = 1e-3)\n",
    "#optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78e9c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration of episode 0 was 128\n",
      "total reward for episode 0 was 128.0\n",
      "duration of episode 100 was 128\n",
      "total reward for episode 100 was 128.0\n",
      "duration of episode 200 was 128\n",
      "total reward for episode 200 was 128.0\n",
      "duration of episode 300 was 128\n",
      "total reward for episode 300 was 128.0\n",
      "duration of episode 400 was 128\n",
      "total reward for episode 400 was 128.0\n",
      "duration of episode 500 was 128\n",
      "total reward for episode 500 was 128.0\n",
      "duration of episode 600 was 128\n",
      "total reward for episode 600 was 128.0\n",
      "duration of episode 700 was 128\n",
      "total reward for episode 700 was 128.0\n",
      "duration of episode 800 was 128\n",
      "total reward for episode 800 was 128.0\n",
      "duration of episode 900 was 128\n",
      "total reward for episode 900 was 128.0\n",
      "duration of episode 1000 was 128\n",
      "total reward for episode 1000 was 128.0\n",
      "duration of episode 1100 was 128\n",
      "total reward for episode 1100 was 128.0\n",
      "duration of episode 1200 was 128\n",
      "total reward for episode 1200 was 128.0\n",
      "duration of episode 1300 was 128\n",
      "total reward for episode 1300 was 128.0\n",
      "duration of episode 1400 was 128\n",
      "total reward for episode 1400 was 128.0\n",
      "duration of episode 1500 was 128\n",
      "total reward for episode 1500 was 128.0\n",
      "duration of episode 1600 was 128\n",
      "total reward for episode 1600 was 128.0\n",
      "duration of episode 1700 was 128\n",
      "total reward for episode 1700 was 128.0\n",
      "duration of episode 1800 was 128\n",
      "total reward for episode 1800 was 128.0\n",
      "duration of episode 1900 was 128\n",
      "total reward for episode 1900 was 128.0\n",
      "duration of episode 2000 was 128\n",
      "total reward for episode 2000 was 128.0\n",
      "duration of episode 2100 was 128\n",
      "total reward for episode 2100 was 128.0\n",
      "duration of episode 2200 was 128\n",
      "total reward for episode 2200 was 128.0\n",
      "duration of episode 2300 was 128\n",
      "total reward for episode 2300 was 128.0\n",
      "duration of episode 2400 was 128\n",
      "total reward for episode 2400 was 128.0\n",
      "duration of episode 2500 was 128\n",
      "total reward for episode 2500 was 128.0\n",
      "duration of episode 2600 was 128\n",
      "total reward for episode 2600 was 128.0\n",
      "duration of episode 2700 was 128\n",
      "total reward for episode 2700 was 128.0\n",
      "duration of episode 2800 was 128\n",
      "total reward for episode 2800 was 128.0\n",
      "duration of episode 2900 was 128\n",
      "total reward for episode 2900 was 128.0\n",
      "duration of episode 3000 was 128\n",
      "total reward for episode 3000 was 128.0\n",
      "duration of episode 3100 was 128\n",
      "total reward for episode 3100 was 128.0\n",
      "duration of episode 3200 was 128\n",
      "total reward for episode 3200 was 128.0\n",
      "duration of episode 3300 was 128\n",
      "total reward for episode 3300 was 128.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "n_ep = 8000\n",
    "batch_size = 128\n",
    "\n",
    "train(linear, env, sgd, n_ep, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0384799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(model, episodes, show = False, plot = False):\n",
    "\n",
    "    if show:\n",
    "        game = gym.make('CartPole-v1', render_mode = \"human\")\n",
    "    else:\n",
    "        game = gym.make('CartPole-v1')\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        obs, info = game.reset()\n",
    "        obs = torch.from_numpy(obs)\n",
    "        t_r = 0.0\n",
    "        duration = 0\n",
    "        while True:\n",
    "            time.sleep(0.01)\n",
    "           # action = model(obs).argmax().item()\n",
    "            action = model.act(obs)\n",
    "            obs, reward, terminated, truncated, info = game.step(action)\n",
    "            obs = torch.from_numpy(obs)\n",
    "            if terminated or truncated:\n",
    "                print(f\"duration of episode {ep} was {duration}\")\n",
    "                print(f\"total reward for episode {ep} was {t_r}\")\n",
    "                obs, info = game.reset()\n",
    "                break\n",
    "            duration += 1\n",
    "            t_r += reward\n",
    "        rewards.append(t_r)\n",
    "    plt.plot(rewards)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "play(linear, 15, False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_m = random_model()\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "play(r_m,10,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e8955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dade3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73bf624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "619c238c2d76aa78136237a3f736491236b937e4fb19f16806a3270802d7c63c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
